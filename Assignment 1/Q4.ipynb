{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "FeedForwardNet(Assignment 1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.6 64-bit"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IE 643: Feed Forward Net (Multi-Layered Perceptron) Demo"
      ],
      "metadata": {
        "id": "meRnMrRlZTH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this demo, we will learn how to build a Multi-layered Perceptron (Feed Forward Nerual Network) model from scratch. There are a lot of libraries available like Pytorch, Keras, Tensorflow etc. which can help you build much complex models in much fewer lines. However before using them, it is important to understand how to build your own neural network from scratch. "
      ],
      "metadata": {
        "id": "lJ13GRM5Hu57"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "np.random.seed(0)"
      ],
      "outputs": [],
      "metadata": {
        "id": "QT3Suc9U_yq8",
        "scrolled": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data loading and pre-processing**\r\n",
        "\r\n",
        "We will be using the MNIST dataset for classification. It contains grayscale images of handwritten digits(0-9) in 28x28 size. Our task is to build a neural network to classify these images. \r\n",
        "\r\n",
        "> The MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\r\n",
        "It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.\r\n",
        "\r\n",
        "Source: http://yann.lecun.com/exdb/mnist/\r\n",
        "\r\n",
        "## Loading the dataset:"
      ],
      "metadata": {
        "id": "F1A1MYFw_yrG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "X, y = None, None\r\n",
        "\r\n",
        "with open(\"IMDB_feats.txt\") as file_in:\r\n",
        "    lines = []\r\n",
        "    N_features = 0\r\n",
        "    N_samples = 0\r\n",
        "    for line in file_in:\r\n",
        "        N_samples += 1\r\n",
        "        pairs = line.split()\r\n",
        "        for pair in pairs:\r\n",
        "            [key, val] = pair.split(':')\r\n",
        "            key, val = int(key), int(val)\r\n",
        "            N_features = max(N_features, key)\r\n",
        "    \r\n",
        "    X = np.zeros((N_samples, N_features))\r\n",
        "    i = 0\r\n",
        "    for line in file_in:\r\n",
        "        N_samples += 1\r\n",
        "        pairs = line.split()\r\n",
        "        for pair in pairs:\r\n",
        "            [key, val] = pair.split(':')\r\n",
        "            key, val = int(key), int(val)\r\n",
        "            X[i][key] = 1\r\n",
        "        i += 1\r\n",
        "\r\n",
        "with open(\"IMDB_labels.txt\") as file_in:\r\n",
        "    lines = []\r\n",
        "    N_features = 0\r\n",
        "    N_samples = 0\r\n",
        "    for line in file_in:\r\n",
        "        N_samples += 1\r\n",
        "        pairs = line.split()\r\n",
        "        for pair in pairs:\r\n",
        "            [key, val] = pair.split(':')\r\n",
        "            key, val = int(key), int(val)\r\n",
        "            N_features = max(N_features, key)\r\n",
        "\r\n",
        "    y = np.zeros((N_samples, N_features))\r\n",
        "    i = 0\r\n",
        "    for line in file_in:\r\n",
        "        N_samples += 1\r\n",
        "        pairs = line.split()\r\n",
        "        for pair in pairs:\r\n",
        "            [key, val] = pair.split(':')\r\n",
        "            key, val = int(key), int(val)\r\n",
        "            X[i][key] = 1\r\n",
        "        i += 1\r\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "print(X.shape)\r\n",
        "print(y.shape)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(120919, 1000)\n",
            "(120919, 27)\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def train_validate_test_split(df, train_percent=0.7, validate_percent=0.15, seed=None):\r\n",
        "    np.random.seed(seed)\r\n",
        "    perm = np.random.permutation(df.index)\r\n",
        "    m = len(df.index)\r\n",
        "    train_end = int(train_percent * m)\r\n",
        "    validate_end = int(validate_percent * m) + train_end\r\n",
        "    train = df.iloc[perm[:train_end]]\r\n",
        "    validate = df.iloc[perm[train_end:validate_end]]\r\n",
        "    test = df.iloc[perm[validate_end:]]\r\n",
        "    \r\n",
        "    return train, validate, test"
      ],
      "outputs": [],
      "metadata": {}
    }
  ]
}