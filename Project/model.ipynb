{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "import torch\r\n",
    "from torch import nn, optim\r\n",
    "from torch.autograd import Variable\r\n",
    "from torchvision import datasets, transforms\r\n",
    "import matplotlib.pyplot as plt\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "print(device)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "train_dataset = datasets.MNIST(\r\n",
    "    root=\"./data\", train=True, transform=transforms.ToTensor(), download=True\r\n",
    ")\r\n",
    "\r\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transforms.ToTensor())\r\n",
    "\r\n",
    "batch_size = 100\r\n",
    "n_epochs = 20\r\n",
    "# n_iters = 3000\r\n",
    "# num_epochs = n_iters / (len(train_dataset) / batch_size)\r\n",
    "# num_epochs = int(num_epochs)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "train_loader = torch.utils.data.DataLoader(\r\n",
    "    dataset=train_dataset, batch_size=batch_size, shuffle=True\r\n",
    ")\r\n",
    "\r\n",
    "# valid_loader = torch.utils.data.DataLoader(\r\n",
    "#     dataset=valid_dataset, batch_size=batch_size, shuffle=False\r\n",
    "# )\r\n",
    "\r\n",
    "test_loader = torch.utils.data.DataLoader(\r\n",
    "    dataset=test_dataset, batch_size=batch_size, shuffle=False\r\n",
    ")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# coRNN Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# class coRNNCell(nn.Module):\r\n",
    "#     def __init__(self, input_size, hidden_size, dt, gamma, epsilon):\r\n",
    "#         super().__init__()\r\n",
    "#         self.dt = dt\r\n",
    "#         self.gamma = gamma\r\n",
    "#         self.epsilon = epsilon\r\n",
    "#         self.i2h = nn.Linear(input_size + hidden_size + hidden_size, hidden_size)\r\n",
    "\r\n",
    "#     def forward(self, x, hy, hz):\r\n",
    "#         combined_layer = torch.cat((x, hz, hy), 1)\r\n",
    "#         hz = hz + self.dt * (\r\n",
    "#             torch.tanh(self.i2h(combined_layer)) - self.gamma * hy - self.epsilon * hz\r\n",
    "#         )\r\n",
    "#         hy = hy + self.dt * hz\r\n",
    "# \r\n",
    "#         return hy, hz\r\n",
    "\r\n",
    "\r\n",
    "# class coRNN(nn.Module):\r\n",
    "#     def __init__(self, input_size, hidden_size, output_size, dt, gamma, epsilon):\r\n",
    "#         super().__init__()\r\n",
    "#         self.hidden_size = hidden_size\r\n",
    "#         self.cell = coRNNCell(input_size, hidden_size, dt, gamma, epsilon)\r\n",
    "#         self.readout = nn.Linear(hidden_size, output_size)\r\n",
    "\r\n",
    "#     def forward():\r\n",
    "#         hy = Variable(torch.zeros(batch_size, hidden_size))\r\n",
    "#         hz = Variable\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LSTM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "class LSTMModel(nn.Module):\r\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        # ? Hidden layer dimentions\r\n",
    "        self.hidden_dim = hidden_dim\r\n",
    "        self.n_layers = n_layers\r\n",
    "\r\n",
    "        # ? shape: (batch, seq, feature)\r\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\r\n",
    "\r\n",
    "        # ? Readout layer\r\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        # ? x shape: (batch_size, seq_len, input_size)\r\n",
    "        # ? h0 shape: (n_layers, batch_size, hidden_dim)\r\n",
    "        h0 = Variable(torch.zeros(self.n_layers, x.size(0), self.hidden_dim))\r\n",
    "        c0 = Variable(torch.zeros(self.n_layers, x.size(0), self.hidden_dim))\r\n",
    "\r\n",
    "        h0, c0 = h0.to(device), c0.to(device)\r\n",
    "\r\n",
    "        # ? out shape: (seq_len, batch_size, hidden_dim) => (64, 28, 100)\r\n",
    "        # ? hn shape: (n_layers * num_directions, batch_size, hidden_dim)\r\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\r\n",
    "        out = self.fc(out[:, -1, :])\r\n",
    "\r\n",
    "        return out\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "class GRUModel(nn.Module):\r\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, dropout=0.2):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        # ? Hidden layer dimentions\r\n",
    "        self.hidden_dim = hidden_dim\r\n",
    "        self.n_layers = n_layers\r\n",
    "\r\n",
    "        # ? shape: (batch, seq, feature)\r\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True)\r\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        h0 = Variable(torch.zeros(self.n_layers, x.size(0), self.hidden_dim)).to(device)\r\n",
    "\r\n",
    "        out, hn = self.gru(x, h0)\r\n",
    "        out = self.fc(out[:, -1, :])\r\n",
    "\r\n",
    "        return out\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "input_dim = 28\r\n",
    "hidden_dim = 100\r\n",
    "output_dim = 10\r\n",
    "n_layers = 3\r\n",
    "\r\n",
    "seq_dim = 28\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "def train(model, train_loader, optimizer, criterion):\r\n",
    "    model.train()\r\n",
    "\r\n",
    "    total_loss = 0\r\n",
    "\r\n",
    "    for data, target in train_loader:\r\n",
    "        images, labels = Variable(data), Variable(target)\r\n",
    "        images, labels = images.to(device), labels.to(device)\r\n",
    "\r\n",
    "        images = images.view(-1, seq_dim, input_dim)\r\n",
    "\r\n",
    "        # ? Forward pass\r\n",
    "        # * outputs size = (100, 10)\r\n",
    "        outputs = model(images)\r\n",
    "        loss = criterion(outputs, labels)\r\n",
    "\r\n",
    "        # ? Backward and optimize\r\n",
    "        optimizer.zero_grad()\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "\r\n",
    "        total_loss += loss.item()\r\n",
    "\r\n",
    "    return total_loss/len(train_loader)\r\n",
    "\r\n",
    "\r\n",
    "def validate(model, valid_loader):\r\n",
    "    correct = 0\r\n",
    "    total = 0\r\n",
    "    model.eval()\r\n",
    "\r\n",
    "    for images, labels in valid_loader:\r\n",
    "        images = Variable(images.view(-1, seq_dim, input_dim)).to(device)\r\n",
    "\r\n",
    "        outputs = model(images)\r\n",
    "\r\n",
    "        # Get predictions from the maximum value\r\n",
    "        _, predicted = torch.max(outputs.data, 1)\r\n",
    "\r\n",
    "        # Total number of labels\r\n",
    "        total += labels.size(0)\r\n",
    "\r\n",
    "        correct += (predicted.cpu() == labels.cpu()).sum()\r\n",
    "\r\n",
    "    accuracy = 100 * correct / total\r\n",
    "    \r\n",
    "    return accuracy\r\n",
    "\r\n",
    "def predict(model, image):\r\n",
    "    model.eval()\r\n",
    "    outputs = model(image)\r\n",
    "    _, predicted = torch.max(outputs.data, 1)\r\n",
    "\r\n",
    "    return predicted\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training LSTM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "lstm_model = LSTMModel(input_dim, hidden_dim, output_dim, n_layers)\r\n",
    "lstm_model.to(device)\r\n",
    "\r\n",
    "criterion = nn.CrossEntropyLoss()\r\n",
    "\r\n",
    "learning_rate = 0.1\r\n",
    "optimizer = optim.SGD(lstm_model.parameters(), lr=learning_rate)  \r\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "lstm_valid_accuracy = []\r\n",
    "lstm_train_loss = []\r\n",
    "\r\n",
    "for epoch in range(n_epochs):\r\n",
    "    loss = train(lstm_model, train_loader, optimizer, criterion)\r\n",
    "    lstm_train_loss.append(loss)\r\n",
    "\r\n",
    "    accuracy = validate(lstm_model, test_loader)\r\n",
    "    lstm_valid_accuracy.append(accuracy)\r\n",
    "\r\n",
    "    print(f\"epoch: {epoch+1}, training-loss: {loss}, validation-accuracy: {accuracy}\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0, training-loss: 2.3013115266958875, validation-accuracy: 11.350000381469727\n",
      "Epoch: 1, training-loss: 2.2989475870132448, validation-accuracy: 11.350000381469727\n",
      "Epoch: 2, training-loss: 2.0348194295167925, validation-accuracy: 34.88999938964844\n",
      "Epoch: 3, training-loss: 0.9805474570890268, validation-accuracy: 78.44000244140625\n",
      "Epoch: 4, training-loss: 0.4793452968945106, validation-accuracy: 88.97000122070312\n",
      "Epoch: 5, training-loss: 0.266025371948878, validation-accuracy: 94.41000366210938\n",
      "Epoch: 6, training-loss: 0.18216475713377198, validation-accuracy: 96.2300033569336\n",
      "Epoch: 7, training-loss: 0.13911955958853164, validation-accuracy: 96.63999938964844\n",
      "Epoch: 8, training-loss: 0.11743276687183728, validation-accuracy: 97.41999816894531\n",
      "Epoch: 9, training-loss: 0.09946455820308377, validation-accuracy: 97.36000061035156\n",
      "Epoch: 10, training-loss: 0.08752988466216872, validation-accuracy: 97.31999969482422\n",
      "Epoch: 11, training-loss: 0.08109535409137607, validation-accuracy: 97.41999816894531\n",
      "Epoch: 12, training-loss: 0.07169368132095164, validation-accuracy: 97.69000244140625\n",
      "Epoch: 13, training-loss: 0.06549332614677648, validation-accuracy: 97.95999908447266\n",
      "Epoch: 14, training-loss: 0.05834722746629268, validation-accuracy: 98.05999755859375\n",
      "Epoch: 15, training-loss: 0.05611563343554735, validation-accuracy: 98.13999938964844\n",
      "Epoch: 16, training-loss: 0.05083020364594025, validation-accuracy: 98.04000091552734\n",
      "Epoch: 17, training-loss: 0.04709941693077174, validation-accuracy: 98.20999908447266\n",
      "Epoch: 18, training-loss: 0.044832091797919325, validation-accuracy: 98.37000274658203\n",
      "Epoch: 19, training-loss: 0.03891812451765873, validation-accuracy: 98.16000366210938\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training GRU"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "gru_model = GRUModel(input_dim, hidden_dim, output_dim, n_layers)\r\n",
    "gru_model.to(device)\r\n",
    "\r\n",
    "criterion = nn.CrossEntropyLoss()\r\n",
    "\r\n",
    "learning_rate = 0.1\r\n",
    "optimizer = optim.SGD(gru_model.parameters(), lr=learning_rate)  \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "gru_valid_accuracy = []\r\n",
    "gru_train_loss = []\r\n",
    "\r\n",
    "for epoch in range(n_epochs):\r\n",
    "    loss = train(gru_model, train_loader, optimizer, criterion)\r\n",
    "    gru_train_loss.append(loss)\r\n",
    "\r\n",
    "    accuracy = validate(gru_model, test_loader)\r\n",
    "    gru_valid_accuracy.append(accuracy)\r\n",
    "\r\n",
    "    print(f\"epoch: {epoch+1}, training-loss: {loss}, validation-accuracy: {accuracy}\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0, training-loss: 1.6613507726788521, validation-accuracy: 73.44999694824219\n",
      "Epoch: 1, training-loss: 0.5151151708761851, validation-accuracy: 89.80000305175781\n",
      "Epoch: 2, training-loss: 0.2471583192795515, validation-accuracy: 94.23999786376953\n",
      "Epoch: 3, training-loss: 0.16029263036015132, validation-accuracy: 95.55999755859375\n",
      "Epoch: 4, training-loss: 0.11858338850550354, validation-accuracy: 96.94999694824219\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plots"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\r\n",
    "\r\n",
    "ax[0][0].plot(lstm_train_loss)\r\n",
    "ax[0][0].plot(gru_train_loss)\r\n",
    "\r\n",
    "ax[0][1].plot(lstm_valid_accuracy)\r\n",
    "ax[0][1].plot(gru_valid_accuracy)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}